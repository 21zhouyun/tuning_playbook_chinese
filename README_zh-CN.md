# 深度学习调优手册

**这不是官方支持的 Google 产品。**

**Varun Godbole<sup>†</sup>，George E. Dahl<sup>†</sup>，贾斯汀 · 吉尔默<sup>†</sup>，克里斯托弗 ·J· 沙鲁<sup>‡</sup> , Zachary Nado<sup>†</sup>**


†谷歌研究，大脑团队

‡哈佛大学

## 目录

-   [这份文件是为谁准备的？](#who-is-this-document-for)
-   [为什么要调优手册？](#why-a-tuning-playbook)
-   [开始新项目的指南](#guide-for-starting-a-new-project)
    -   [选择模型架构](#choosing-a-model-architecture)
    -   [选择优化器](#choosing-the-optimizer)
    -   [选择批量大小](#choosing-the-batch-size)
    -   [选择初始配置](#choosing-the-initial-configuration)
-   [提高模型性能的科学方法](#a-scientific-approach-to-improving-model-performance)
    -   [增量调整策略](#the-incremental-tuning-strategy)
    -   [探索与开发](#exploration-vs-exploitation)
    -   [选择下一轮实验的目标](#choosing-the-goal-for-the-next-round-of-experiments)
    -   [设计下一轮实验](#Designing-the-next-round-of-experiments)
    -   [确定是否采用训练管道更改或超参数配置](#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration)
    -   [探索结束后](#After-exploration-concludes)
-   [确定每次训练运行的步数](#Determining-the-number-of-steps-for-each-training-run)
    -   [在训练不受计算限制时决定训练多长时间](#Deciding-how-long-to-train-when-training-is-not-compute-bound)
    -   [在训练受计算限制时决定训练多长时间](#Deciding-how-long-to-train-when-training-is-compute-bound)
-   [训练管道的附加指南](#Additional-guidance-for-the-training-pipeline)
    -   [优化输入管道](#Optimizing-the-input-pipeline)
    -   [评估模型性能](Evaluating-model-performance)
    -   [保存检查点并追溯选择最佳检查点](#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint)
    -   [设置实验跟踪](#Setting-up-experiment-tracking)
    -   [批量归一化实现细节](#Batch-normalization-implementation-details)
    -   [多主机管道的注意事项](#Considerations-for-multi-host-pipelines)
-   [常见问题解答](#faqs)
-   [致谢](# 致谢)
-   [引用](# 引用)
-   [贡献](#contributing)

## 这份文件是为谁准备的？

本文档适用于对**最大化深度学习模型的性能**感兴趣的工程师和研究人员（包括个人和团队）。我们假定您具备机器学习和深度学习概念的基本知识。

我们的重点是**超参数调整的过程**。我们触及了深度学习训练的其他方面，例如管道实施和优化，但我们对这些方面的处理并不打算完成。

我们假设机器学习问题是一个监督学习问题或看起来很像的问题（例如自我监督）。也就是说，本文档中的一些规定也可能适用于其他类型的问题。

## 为什么要调优手册？

目前，要使深度神经网络在实践中正常运行，需要付出大量的努力和猜测。更糟糕的是，很少有人记录人们用来通过深度学习获得良好结果的实际方法。论文掩盖了导致最终结果的过程，以呈现一个更清晰的故事，而处理商业问题的机器学习工程师很少有时间退后一步并概括他们的过程。教科书倾向于回避实用指导并优先考虑基本原则，即使它们的作者在应用工作方面具有必要的经验以提供有用的建议。在准备创建此文档时，我们找不到任何全面的尝试来实际解释*如何通过深度学习获得好的结果*。相反，我们在博客文章和社交媒体上发现了一些建议片段，从研究论文的附录中偷窥的技巧，关于一个特定项目或管道的偶尔案例研究，以及很多混乱。深度学习专家与使用表面相似方法的低技能从业者取得的成果之间存在巨大鸿沟。与此同时，这些专家很乐意承认他们所做的一些事情可能没有充分的理由。随着深度学习的成熟并对世界产生更大的影响，社区需要更多涵盖有用方法的资源，包括对于获得良好结果至关重要的所有实用细节。

我们是一个由五名研究人员和工程师组成的团队，他们在深度学习领域工作多年，其中一些人早在 2006 年就开始了。我们已经将深度学习应用于从语音识别到天文学的各个领域的问题，并在此过程中学到了很多东西. 本文档源于我们自己训练神经网络、教授新机器学习工程师以及为我们的同事提供深度学习实践建议的经验。虽然很高兴看到深度学习从少数学术实验室实践的机器学习方法发展为数十亿人使用的技术驱动产品，但深度学习作为一门工程学科仍处于起步阶段，我们希望本文档鼓励其他人帮助系统化该领域的实验协议。

这份文件是在我们试图明确我们自己的深度学习方法时产生的，因此它代表了作者在撰写本文时的观点，而不是任何客观事实。我们自己在超参数调整方面的努力使其成为我们指导的特别重点，但我们也涵盖了我们在工作中遇到的其他重要问题（或看到出错）。我们的目的是让这项工作成为一个活的文件，随着我们信念的改变而成长和演变。例如，关于调试和减少训练失败的材料对我们来说在两年前是不可能写的，因为它是基于最近的结果和正在进行的调查。不可避免地，我们的一些建议将需要更新以考虑新的结果和改进的工作流程。我们不知道*最佳的*深度学习秘诀，但在社区开始记录和讨论不同的程序之前，我们不可能找到它。为此，我们鼓励发现我们的建议存在问题的读者提出替代建议以及令人信服的证据，以便我们更新剧本。我们也希望看到可能有不同建议的替代指南和剧本，以便我们可以作为一个社区努力实现最佳实践。最后，任何标有🤖表情符号的部分都是我们想要做更多研究的地方。只有在尝试编写这本 playbook 之后，才完全清楚在深度学习从业者的工作流程中可以找到多少有趣和被忽视的研究问题。

## 开始新项目的指南

我们在调整过程中做出的许多决定都可以在项目开始时做出一次，只有在情况发生变化时才会偶尔重新考虑。

我们在下面的指南中做出了以下假设：

-   问题制定、数据清理等基本工作已经完成，花时间在模型架构和训练配置上是有意义的。
-   已经有一个管道设置来进行训练和评估，并且可以很容易地为各种感兴趣的模型执行训练和预测工作。
-   已选择并实施适当的指标。这些应该尽可能地代表在部署环境中测量的内容。

### 选择模型架构

***概括：*** *在开始一个新项目时，尝试重用已经可用的模型。*

-   选择一个完善的、常用的模型架构首先开始工作。以后总是可以构建自定义模型。
-   模型架构通常具有各种超参数，用于确定模型的大小和其他细节（例如层数、层宽度、激活函数类型）。
    -   因此，选择架构实际上意味着选择一系列不同的模型（一个用于模型超参数的每个设置）。
    -   我们将考虑在[选择初始配置](#choosing-the-initial-configuration)和[提高模型性能的科学方法](#a-scientific-approach-to-improving-model-performance)中选择模型超参数的问题。
-   如果可能，请尝试找到一篇解决尽可能接近手头问题的论文，并将该模型作为起点进行重现。

### 选择优化器

***概括：*** *从针对手头问题类型的最流行的优化器开始。*

-   在所有类型的机器学习问题和模型架构中，没有优化器是“最好的”。甚至只是[比较优化器的性能是一项艰巨的任务](https://arxiv.org/abs/1910.05446)。🤖
-   我们建议坚持使用成熟、流行的优化器，尤其是在开始新项目时。
    -   理想情况下，选择用于同一类型问题的最流行的优化器。
-   准备好关注所选优化器的**\***** 全部****\*** 超参数。
    -   具有更多超参数的优化器可能需要更多的调整工作才能找到最佳配置。
    -   当我们试图找到各种其他超参数（例如架构超参数）的最佳值同时将优化器超参数视为[讨厌的参数](#identifying-scientific-nuisance-and-fixed-hyperparameters)时，这在项目的开始阶段尤其重要。
    -   从更简单的优化器开始可能更可取（例如，具有固定动量的 SGD 或具有固定$\epsilon$、$\beta_{1}$和$\beta_{2}$的 Adam ) 在项目的初始阶段，稍后切换到更通用的优化器。
-   我们喜欢的完善的优化器包括（但不限于）：
    -   [具有势头的新元](#what-are-the-update-rules-for-all-the-popular-optimization-algorithms)（我们喜欢 Nesterov 变体）
    -   [亚当和那亚当](#what-are-the-update-rules-for-all-the-popular-optimization-algorithms)，比具有动量的 SGD 更通用。请注意，Adam 有 4 个可调超参数[他们都很重要](https://arxiv.org/abs/1910.05446)！
        -   参见[Adam 的超参数应该如何调整？](#how-should-adams-hyperparameters-be-tuned)

### 选择批量大小

***概括：*** *批量大小决定训练速度，不应用于直接调整验证集性能。通常，理想的批量大小将是可用硬件支持的最大批量大小。*

-   批量大小是决定*训练时间*和*计算资源消耗*的关键因素。
-   增加批量大小通常会减少训练时间。这可能非常有益，因为它，例如：
    -   允许在固定时间间隔内更彻底地调整超参数，可能会产生更好的最终模型。
    -   减少开发周期的延迟，允许更频繁地测试新想法。
-   增加批大小可能会减少、增加或不改变资源消耗。
-   批量大小应*不是*视为验证集性能的可调超参数。
    -   只要调整好所有超参数（尤其是学习率和正则化超参数）并且训练步数足够，使用任何批量大小都应该可以获得相同的最终性能（参见[沙鲁等人。2018](https://arxiv.org/abs/1811.03600)） .
    -   请参阅[为什么不应该调整批量大小来直接提高验证集性能？](#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance)

#### 确定可行的批量大小并估计训练吞吐量


<details><summary><em>[点击展开]</em></summary>


<br>

-   对于给定的模型和优化器，可用硬件通常支持一系列批量大小。限制因素通常是加速器内存。
-   不幸的是，如果不运行或至少编译完整的训练程序，就很难计算出哪些批量大小适合内存。
-   最简单的解决方案通常是以不同的批次大小（例如，增加 2 的幂）运行少量步骤的训练作业，直到其中一个作业超过可用内存。
-   对于每个批量大小，我们应该训练足够长的时间以获得*训练吞吐量*的可靠估计

<p align="center">训练吞吐量 =（# 每秒处理的例子）</p>

<p align="center">或者，等效地，<em>每步时间</em>。</p>

<p align="center">每步时间 =（批量大小）/（训练吞吐量）</p>

-   当加速器尚未饱和时，如果批量大小加倍，训练吞吐量也应该加倍（或至少接近加倍）。等效地，随着批量大小的增加，每步的时间应该是恒定的（或至少接近恒定的）。
-   如果不是这种情况，则训练管道存在瓶颈，例如 I/O 或计算节点之间的同步。这可能值得在继续之前进行诊断和纠正。
-   如果训练吞吐量仅增加到某个最大批大小，那么我们应该只考虑达到该最大批大小的批大小，即使硬件支持更大的批大小也是如此。
    -   使用更大批量大小的所有好处都假定训练吞吐量增加。如果没有，请修复瓶颈或使用较小的批量大小。
    -   **梯度积累**模拟比硬件可以支持的更大的批量大小，因此不提供任何吞吐量优势。在应用工作中通常应避免使用它。
-   每次更改模型或优化器时，可能都需要重复这些步骤（例如，不同的模型架构可能允许更大的批次大小适合内存）。

</details>

#### 选择批量大小以最小化训练时间

<details><summary><em>[点击展开]</em></summary>


<br>


<p align="center">训练时间 =（每步时间）x（总步数）</p>

-   对于所有可行的批量大小，我们通常可以认为每步的时间近似恒定。当没有来自并行计算的开销并且所有训练瓶颈都已被诊断和纠正时，这是正确的（有关如何识别训练瓶颈的信息，请参阅[上一节](#determining-the-feasible-batch-sizes-and-estimating-training-throughput)）。实际上，增加批量大小通常至少会产生一些开销。
-   随着批量大小的增加，达到固定性能目标所需的总步数通常会减少（前提是在更改批量大小时重新调整所有相关超参数；[沙鲁等人。2018](https://arxiv.org/abs/1811.03600)）。
    -   例如，将批量大小加倍可能会使所需的总步骤数减半。这称为**完美缩放**。
    -   完美缩放适用于所有批次大小直至临界批次大小，超过该批次大小将实现收益递减。
    -   最终，增加批量大小不再减少训练步骤的数量（但永远不会增加）。
-   因此，最小化训练时间的批量大小通常是最大的批量大小，仍然可以减少所需的训练步骤数。
    -   这个批量大小取决于数据集、模型和优化器，除了通过实验为每个新问题找到它之外，如何计算它是一个悬而未决的问题。🤖
    -   比较批量大小时，请注意示例预算/[epoch](https://developers.google.com/machine-learning/glossary#epoch)预算（运行所有实验，同时固定训练示例演示文稿的数量）和步长预算（运行具有训练步骤数的所有实验）之间的区别固定的）。
        -   将批量大小与 epoch 预算进行比较只会探索完美的扩展机制，即使更大的批量大小仍可能通过减少所需的训练步骤数来提供有意义的加速。
    -   通常，可用硬件支持的最大批大小将小于关键批大小。因此，一个好的经验法则（不运行任何实验）是使用尽可能大的批量大小。
-   如果最终增加了训练时间，那么使用更大的批量大小就没有意义了。

</details>

#### 选择批量大小以最小化资源消耗

<details><summary><em>[点击展开]</em></summary>


<br>


-   有两种类型的资源成本与增加批量大小相关：
    1.  *预付费用*，例如购买新硬件或重写训练管道以实现多 GPU/多 TPU 训练。
    2.  *使用成本*，例如根据团队的资源预算计费，从云提供商处计费，电力/维护成本。
-   如果增加批量大小的前期成本很高，那么最好推迟增加批量大小，直到项目成熟并且更容易评估成本效益权衡。实施多主机并行训练程序可以引入[bugs](#considerations-for-multi-host-pipelines)和[微妙的问题](#batch-normalization-implementation-details)因此无论如何从更简单的管道开始可能更好。（另一方面，在需要大量调整实验的过程早期，训练时间的大幅加速可能非常有益）。
-   我们将总的使用成本（可能包括多种不同的成本）称为“资源消耗”。我们可以将资源消耗分解为以下组件：

<p align="center">资源消耗 =（每步资源消耗）x（总步数）</p>

-   增加批量大小通常允许我们[减少总步数](#choosing-the-batch-size-to-minimize-training-time)。资源消耗是增加还是减少将取决于每步消耗的变化情况。
    -   增加批量大小可能*减少*资源消耗。例如，如果具有较大批量大小的每个步骤都可以在与较小批量大小相同的硬件上运行（每个步骤的时间只增加一点点），那么每个步骤资源消耗的增加可能会被减少所抵消在步数中。
    -   增加批量大小可能*不变*资源消耗。例如，如果将批量大小加倍，所需的步数减半，并且使用的 GPU 数量加倍，则总消耗（以 GPU 小时数表示）不会改变。
    -   增加批量大小可能*增加*资源消耗。例如，如果增加批大小需要升级硬件，则每步消耗的增加可能会超过步数的减少。

</details>

#### 更改批量大小需要重新调整大多数超参数

<details><summary><em>[点击展开]</em></summary>


<br>


-   大多数超参数的最佳值对批量大小敏感。因此，更改批量大小通常需要重新开始调整过程。
-   与批量大小交互最强烈的超参数，因此对于每个批量大小单独调整最重要的是优化器超参数（例如学习率、动量）和正则化超参数。
-   在项目开始时选择批量大小时请记住这一点。如果您以后需要切换到不同的批量大小，则为新的批量大小重新调整所有内容可能会很困难、耗时且成本高昂。

</details>

#### 批量规范如何与批量大小交互

<details><summary><em>[点击展开]</em></summary>


<br>


-   Batch norm 很复杂，一般来说，应该使用与梯度计算不同的 batch size 来计算统计数据。有关详细讨论，请参阅[批量规范部分](#batch-normalization-implementation-details)。

</details>

### 选择初始配置

-   在开始超参数调整之前，我们必须确定起点。这包括指定 (1) 模型配置（例如层数），(2) 优化器超参数（例如学习率），以及 (3) 训练步骤数。
-   确定此初始配置将需要一些手动配置的训练运行和反复试验。
-   我们的指导原则是找到一个简单、相对快速、资源消耗相对较低的配置，以获得“合理”的结果。
    -   “简单”意味着尽可能避免花里胡哨的东西；这些总是可以在以后添加。即使花里胡哨的东西在未来被证明是有用的，但在初始配置中添加它们可能会浪费时间调整无用的功能和/或烘烤不必要的并发症。
        -   例如，在添加花哨的衰减时间表之前以恒定的学习率开始。
    -   选择快速且消耗最少资源的初始配置将使超参数调整更加高效。
        -   例如，从较小的模型开始。
    -   “合理”性能取决于问题，但至少意味着经过训练的模型在验证集上的性能比随机机会好得多（尽管它可能很糟糕，不值得部署）。
-   选择训练步骤的数量涉及平衡以下张力：
    -   一方面，训练更多的步骤可以提高性能并使超参数调整更容易（参见[沙鲁等人。2018](https://arxiv.org/abs/1811.03600)）。
    -   另一方面，更少步骤的训练意味着每次训练运行得更快并且使用更少的资源，通过减少周期之间的时间并允许并行运行更多实验来提高调整效率。此外，如果一开始选择了一个不必要的大步预算，那么以后可能很难改变它，例如，一旦学习率计划针对该步数进行了调整。

## 提高模型性能的科学方法

就本文档而言，机器学习开发的最终目标是最大化已部署模型的效用。尽管开发过程的许多方面因应用程序而异（例如时间长度、可用计算资源、模型类型），但我们通常可以对任何问题使用相同的基本步骤和原则。

我们在下面的指南中做出了以下假设：

-   已经有一个完全运行的训练管道以及获得合理结果的配置。
-   有足够的计算资源可用于进行有意义的调整实验并并行运行至少多个训练作业。

### 增量调整策略

***概括：*** *从简单的配置开始，逐步进行改进，同时深入了解问题。确保任何改进都基于强有力的证据，以避免增加不必要的复杂性。*

-   我们的最终目标是找到一种配置来最大化我们模型的性能。
    -   在某些情况下，我们的目标是在固定截止日期（例如提交给竞赛）之前最大限度地改进模型。
    -   在其他情况下，我们希望无限期地改进模型（例如，不断改进生产中使用的模型）。
-   原则上，我们可以通过使用算法自动搜索可能配置的整个空间来最大化性能，但这不是一个实用的选择。
    -   可能配置的空间非常大，目前还没有任何算法足够复杂，可以在没有人工指导的情况下有效地搜索这个空间。
-   大多数自动搜索算法依赖于定义要搜索的配置集的手工设计的*搜索空间*，这些搜索空间可能非常重要。
-   最大化性能的最有效方法是从简单的配置开始，逐步添加功能并进行改进，同时深入了解问题。
    -   我们在每一轮调整中都使用自动搜索算法，并随着我们理解的增长不断更新我们的搜索空间。
-   随着我们的探索，我们自然会找到越来越好的配置，因此我们的“最佳”模型将不断改进。
    -   当我们更新我们的最佳配置时，我们称之为*发射*（这可能对应也可能不对应于生产模型的实际启动）。
    -   对于每次发布，我们必须确保更改是基于强有力的证据——而不仅仅是基于幸运配置的随机机会——这样我们就不会给训练管道增加不必要的复杂性。

在高层次上，我们的增量调优策略涉及重复以下四个步骤：

1.  为下一轮实验确定范围适当的目标。
2.  设计并运行一组实验，朝着这个目标取得进展。
3.  从结果中了解我们能做什么。
4.  考虑是否推出新的最佳配置。

本节的其余部分将更详细地考虑该策略。

### 探索与开发

***概括：*** *大多数时候，我们的主要目标是深入了解问题。*

-   尽管有人可能认为我们会花费大部分时间来尝试最大化验证集的性能，但实际上我们花费了大部分时间来尝试深入了解问题，而贪婪地关注验证错误的时间相对较少。
    -   也就是说，我们大部分时间都花在了“探索”上，只有一小部分时间花在了“开发”上。
-   从长远来看，如果我们想最大化我们的最终表现，理解问题是至关重要的。将洞察力置于短期收益之上可以帮助我们：
    -   避免仅通过历史事故就在表现良好的运行中进行不必要的更改。
    -   确定验证错误对哪些超参数最敏感，哪些超参数交互最多，因此需要一起重新调整，以及哪些超参数对其他变化相对不敏感，因此可以在未来的实验中修复。
    -   建议尝试使用潜在的新功能，例如在出现过拟合问题时使用新的正则化器。
    -   确定无用的功能，因此可以将其删除，从而降低未来实验的复杂性。
    -   识别来自超参数调整的改进何时可能已经饱和。
    -   围绕最佳值缩小我们的搜索空间，以提高调整效率。
-   当我们最终准备好变得贪婪时，我们可以完全关注验证错误，即使实验没有提供关于调优问题结构的最大信息。

### 选择下一轮实验的目标

***概括：*** *每轮实验都应该有一个明确的目标，并且范围要足够窄，这样实验才能真正朝着目标取得进展。*

-   每轮实验都应该有一个明确的目标，并且范围要足够窄，这样实验才能真正朝着目标取得进展：如果我们试图一次添加多个特征或回答多个问题，我们可能无法理清各自的影响在结果上。
-   示例目标包括：
    -   尝试对管道进行潜在的改进（例如，新的正则化器、预处理选择等）。
    -   了解特定模型超参数（例如激活函数）的影响
    -   贪婪地最大化验证错误。

### 设计下一轮实验

***概括：*** *确定哪些超参数对于实验目标而言是科学的、令人讨厌的和固定的超参数。创建一系列研究以比较科学超参数的不同值，同时优化有害超参数。选择有害超参数的搜索空间，以平衡资源成本与科学价值。*

#### 识别科学的、令人讨厌的和固定的超参数

<details><summary><em>[点击展开]</em></summary>


<br>

-   对于给定的目标，所有超参数都将是**科学超参数**、**讨厌的超参数**或**固定超参数**。
    -   科学超参数是那些对我们试图衡量的模型性能有影响的参数。
    -   讨厌的超参数是那些需要优化的超参数，以便公平地比较科学超参数的不同值。这类似于[讨厌的参数](https://en.wikipedia.org/wiki/Nuisance_parameter)的统计概念。
    -   固定超参数将在当前轮次实验中固定其值。在比较科学超参数的不同值时，这些超参数的值不需要（或者我们不希望它们）改变。
        -   通过为一组实验固定某些超参数，我们必须接受从实验得出的结论可能对固定超参数的其他设置无效。换句话说，固定的超参数对我们从实验中得出的任何结论提出了警告。
-   例如，如果我们的目标是“确定具有更多隐藏层的模型是否会减少验证错误”，那么隐藏层数就是一个科学的超参数。
    -   学习率是一个令人讨厌的超参数，因为如果对每个层数分别调整学习率（最佳学习率通常取决于模型架构），我们只能公平地比较具有不同隐藏层数的模型。
    -   如果我们在之前的实验中确定激活函数的最佳选择对模型深度不敏感，或者如果我们愿意限制我们关于隐藏层数量的结论以仅涵盖该特定选择，则激活函数可以是一个固定的超参数的激活函数。或者，如果我们准备为每个隐藏层数单独调整它，它可能是一个令人讨厌的参数。
-   一个特定的超参数是科学超参数、无用超参数还是固定超参数并不是该超参数固有的，而是根据实验目标而变化的。
    -   例如，激活函数的选择可以是一个科学的超参数（对于我们的问题，ReLU 或 tanh 是更好的选择吗？），一个讨厌的超参数（当我们允许多个模型时，最好的 5 层模型是否优于最好的 6 层模型？不同的可能激活函数？），或一个固定的超参数（对于 ReLU 网络，在特定位置添加批量归一化是否有帮助？）。
-   在设计新一轮实验时，我们首先确定我们实验目标的科学超参数。
    -   在此阶段，我们将所有其他超参数视为讨厌的超参数。
-   接下来，我们将一些讨厌的超参数转换为固定超参数。
    -   有了无限的资源，我们会将所有非科学的超参数保留为讨厌的超参数，这样我们从实验中得出的结论就不会受到关于固定超参数值的警告。
    -   然而，我们尝试调整的讨厌的超参数越多，我们无法针对科学超参数的每个设置充分调整它们并最终从我们的实验中得出错误结论的风险就越大。
        -   如[below](#striking-a-balance-between-informative-and-affordable-experiments)所述，我们可以通过增加计算预算来应对这种风险，但通常我们的最大资源预算低于调整所有非科学超参数所需的资源预算。
    -   我们选择将一个多余的超参数转换为一个固定的超参数，根据我们的判断，修复它所带来的警告比将它作为一个多余的超参数包含在内的成本要小。
        -   给定的有害超参数与科学超参数的交互越多，固定其值的破坏性就越大。例如，权重衰减强度的最佳值通常取决于模型大小，因此假设权重衰减的单个特定值比较不同的模型大小不会很有见地。
-   尽管我们分配给每个超参数的类型取决于实验目标，但对于某些类别的超参数，我们有以下经验法则：
    -   在各种优化器超参数（例如学习率、动量、学习率调度参数、Adam beta 等）中，至少有一些是令人讨厌的超参数，因为它们往往与其他变化的交互作用最大。
        -   它们很少是科学超参数，因为像“当前管道的最佳学习率是多少？”这样的目标。没有给出太多的见解——最好的设置很容易随着下一次管道的改变而改变。
        -   尽管由于资源限制或当我们有特别有力的证据表明它们不与科学参数相互作用时，我们可能偶尔会修复其中一些，但我们通常应该假设优化器超参数必须单独调整以在不同设置之间进行公平比较科学的超参数，因此不应该被固定。
            -   此外，我们没有*先验*理由偏爱一个优化器超参数值而不是另一个（例如，它们通常不会以任何方式影响前向传递或梯度的计算成本）。
    -   相比之下，优化器的*选择*通常是一个科学超参数或固定超参数。
        -   如果我们的实验目标涉及在两个或多个不同的优化器之间进行公平比较（例如“确定哪个优化器在给定的步骤数中产生最低的验证错误”），那么它就是一个科学的超参数。
        -   或者，我们可能出于各种原因将其设为固定的超参数，包括（1）先前的实验使我们相信针对我们问题的最佳优化器对当前的科学超参数不敏感；和/或（2）我们更喜欢使用这个优化器来比较科学超参数的值，因为它的训练曲线更容易推理；和/或 (3) 我们更喜欢使用这个优化器，因为它比其他优化器使用更少的内存。
    -   正则化技术引入的超参数通常是令人讨厌的超参数，但我们是否完全包括正则化技术是一个科学的或固定的超参数。
        -   例如，dropout 增加了代码的复杂性，因此在决定是否包含它时，我们会将“no dropout”与“dropout”作为一个科学的超参数，而将 dropout 率作为一个令人讨厌的超参数。
            -   如果我们决定根据这个实验将 dropout 添加到我们的管道中，那么在未来的实验中，dropout 率将是一个令人讨厌的超参数。
    -   架构超参数通常是科学的或固定的超参数，因为架构变化会影响服务和训练成本、延迟和内存需求。
        -   例如，层数通常是一个科学的或固定的超参数，因为它往往会对训练速度和内存使用产生巨大影响。
-   在某些情况下，干扰和固定超参数集将取决于科学超参数的值。
    -   例如，假设我们试图确定 Nesterov momentum 和 Adam 中哪个优化器的验证错误率最低。科学超参数是 `optimizer`，它的值是 `{"Nesterov_momentum", "Adam"}`。值 `optimizer="Nesterov_momentum"` 引入了干扰/固定超参数 `{learning_rate, momentum}`，但值 `optimizer="Adam"` 引入了干扰/固定超参数 `{learning_rate, beta1, beta2, epsilon}`。
    -   仅针对科学超参数的某些值存在的超参数称为**条件超参数**。
    -   我们不应该仅仅因为两个条件超参数具有相同的名称就认为它们是相同的！在上面的示例中，名为 `learning_rate` 的条件超参数是 `optimizer="Nesterov_momentum"` 与 `optimizer="Adam"` 的*不同的*超参数. 它在两种算法中的作用相似（尽管不完全相同），但在每个优化器中运行良好的值范围通常相差几个数量级。

</details>

#### 创建一组研究

<details><summary><em>[点击展开]</em></summary>


<br>


-   一旦我们确定了科学和令人讨厌的超参数，我们就会设计一个“研究”或一系列研究，以朝着实验目标取得进展。
    -   一项研究指定了一组要运行的超参数配置以供后续分析。每个配置称为“试用”。
    -   创建研究通常涉及选择在试验中会有所不同的超参数，选择这些超参数可以采用的值（“搜索空间”），选择试验次数，以及选择自动搜索算法以从搜索中抽取那么多试验空间。或者，我们可以通过手动指定一组超参数配置来创建研究。
-   研究的目的是使用科学超参数的不同值运行管道，同时**“优化掉”**（或“优化”）讨厌的超参数，以便比较不同的超参数值科学超参数尽可能公平。
-   在最简单的情况下，我们将对科学参数的每个配置进行单独研究，其中每个研究都会调整讨厌的超参数。
    -   例如，如果我们的目标是从 Nesterov momentum 和 Adam 中选择最佳优化器，我们可以创建一个研究，其中 `optimizer="Nesterov_momentum"` 和讨厌的超参数是 `{learning_rate, momentum}`，和另一项研究，其中 `optimizer="Adam"` 和讨厌的超参数是 `{learning_rate, beta1, beta2, epsilon}`。我们将通过从每项研究中选择表现最好的试验来比较这两个优化器。
    -   我们可以使用任何无梯度优化算法，包括贝叶斯优化或进化算法等方法，来优化多余的超参数，尽管[we prefer](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)在[探索阶段](#exploration-vs-exploitation)的调整，因为它在此设置中具有多种优势。[探索结束后](#after-exploration-concludes)，如果有最先进的贝叶斯优化软件可用，那是我们的首选。
-   在更复杂的情况下，我们想要比较大量科学超参数的值，而进行那么多独立研究是不切实际的，我们可以将科学参数包含在与有害超参数相同的搜索空间中，并使用搜索算法在单个研究中对*两个都*科学和讨厌的超参数的值进行采样。
    -   采用这种方法时，条件超参数可能会导致问题，因为很难指定搜索空间，除非有害超参数集对于科学超参数的所有值都相同。
    -   在这种情况下，[我们的偏好](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)用于在更高级的黑盒优化工具上使用准随机搜索甚至更强大，因为它确保我们获得相对均匀的科学超参数值采样。无论搜索算法如何，我们都需要以某种方式确保它统一搜索科学参数。

</details>

#### 在信息丰富和负担得起的实验之间取得平衡

<details><summary><em>[点击展开]</em></summary>


<br>


-   在设计一项研究或一系列研究时，我们需要分配有限的预算，以充分实现以下三个要求：
    1.  比较科学超参数的足够多的不同值。
    2.  在足够大的搜索空间上调整讨厌的超参数。
    3.  对有害超参数的搜索空间进行足够密集的采样。
-   我们越能实现这三个必要条件，我们就能从实验中获得越多的洞察力。
    -   尽可能多地比较科学超参数的值可以拓宽我们从实验中获得的见解的范围。
    -   包括尽可能多的有害超参数并允许每个有害超参数在尽可能大的范围内变化，这增加了我们对每个配置的搜索空间中有害超参数**存在**的“良好”值的信心的科学超参数。
        -   否则，我们可能会通过不搜索有害参数空间的可能区域来对科学超参数的值进行不公平的比较，在这些区域中，科学参数的某些值可能有更好的值。
    -   尽可能密集地对有害超参数的搜索空间进行采样增加了我们的信心，即搜索过程会找到恰好存在于我们的搜索空间中的有害超参数的任何良好设置。
        -   否则，我们可能会在科学参数的值之间进行不公平的比较，因为一些值随着讨厌的超参数的采样而变得更幸运。
-   不幸的是，这三个维度的*任何*的改进需要增加试验次数，从而增加资源成本，或者找到一种方法来节省其他维度之一的资源。
    -   每个问题都有自己的特性和计算限制，因此如何在这三个需求之间分配资源需要一定程度的领域知识。
    -   在进行一项研究后，我们总是试图了解该研究是否将令人讨厌的超参数调整得足够好（即搜索足够大的空间足够广泛）以公平地比较科学超参数（如更详细的描述[below](#extracting-insight-from-experimental-results)）。

</details>

### 从实验结果中提取洞察力

***概括：*** *除了尝试实现每组实验的原始科学目标之外，还要检查其他问题的清单，如果发现问题，请修改实验并重新运行。*

-   最终，每组实验都有一个特定的目标，我们想要评估实验为该目标提供的证据。
    -   然而，如果我们提出正确的问题，我们通常会发现在一组给定的实验能够朝着最初的目标取得很大进展之前需要纠正的问题。
        -   如果我们不问这些问题，我们可能会得出不正确的结论。
    -   由于运行实验的成本很高，我们还希望借此机会从每组实验中提取其他有用的见解，即使这些见解与当前目标并不直接相关。
-   在分析一组给定的实验以朝着最初的目标取得进展之前，我们应该问自己以下额外的问题：
    -   [搜索空间够大吗？](#identifying-bad-search-space-boundaries)
        -   如果研究的最佳点在一维或多维搜索空间的边界附近，则搜索可能不够广泛。在这种情况下，我们应该进行另一项具有扩展搜索空间的研究。
    -   [我们是否从搜索空间中采样了足够多的点？](#not-sampling-enough-points-in-the-search-space)
        -   如果不是，则在调整目标中运行更多点或不那么雄心勃勃。
    -   每项研究中有多少试验是**不可行**（即出现分歧的试验、得到非常糟糕的损失值，或者因为违反某些隐式约束而根本无法运行）？
        -   当研究中很大一部分点是**不可行**时，我们应该尝试调整搜索空间以避免对这些点进行采样，这有时需要重新参数化搜索空间。
        -   在某些情况下，大量不可行点可能表示训练代码中存在错误。
    -   [模型是否存在优化问题？](#how-can-optimization-failures-be-debugged-and-mitigated)
    -   [我们可以从最佳试验的训练曲线中学到什么？](#examining-the-training-curves)
        -   例如，最好的试验是否具有与有问题的过度拟合一致的训练曲线？
-   如有必要，根据上述问题的答案，改进最近的研究（或研究组）以改进搜索空间和/或抽样更多试验，或采取其他一些纠正措施。
-   一旦我们回答了上述问题，我们就可以继续评估实验为我们最初的目标提供的证据（例如，[评估改变是否有用](#detecting-whether-a-change-is-useful-with-isolation-plots)）。

#### 识别错误的搜索空间边界

<details><summary><em>[点击展开]</em></summary>


<br>


-   如果从中采样的最佳点靠近其边界，则搜索空间是可疑的。如果我们朝那个方向扩大搜索范围，我们可能会找到更好的点。
-   为了检查搜索空间边界，我们喜欢在我们称之为**基本超参数轴图**的地方绘制已完成的试验，我们在其中绘制验证目标值与其中一个超参数（例如学习率）的关系。图中的每个点都对应于一次试验。
    -   每次试验的验证目标值通常应该是它在培训过程中获得的最佳值。

<p align="center" id="figure-1">
<img src="assets/bad_search_space.png" width="49%" alt="Example of bad search space boundaries">
<img src="assets/good_search_space.png" width="49%" alt="Example of good search space boundaries">
</p>


<p align="center"><b>图 1：</b>不良搜索空间边界和可接受的搜索空间边界示例。</p>

-   [Figure 1](#figure-1)中的图表显示错误率（越低越好）与初始学习率的关系。
-   如果最佳点聚集在搜索空间的边缘（在某个维度上），则可能需要扩展搜索空间边界，直到最佳观察点不再靠近边界。
-   通常，一项研究将包括“不可行”的试验，这些试验会产生分歧或得到非常糟糕的结果（在上图中用红色 X 标记）。
    -   如果所有试验对于大于某个阈值的学习率都是不可行的，并且如果表现最好的试验在该区域的边缘具有学习率，则模型[可能会遇到稳定性问题，无法获得更高的学习率](#how-can-optimization-failures-be-debugged-and-mitigated)。

</details>

#### 没有在搜索空间中采样足够的点

<details><summary><em>[点击展开]</em></summary>


<br>


-   一般来说，[可能很难知道](#how-many-trials-are-needed-to-get-good-results-with-quasi-random-search)如果搜索空间已经被足够密集地采样。🤖
-   运行更多的试验当然更好，但代价是显而易见的。
-   由于很难知道我们什么时候采样足够，我们通常会采样我们可以负担得起的东西，并尝试通过反复查看各种超参数轴图来校准我们的直觉信心，并试图了解有多少点处于“良好”状态。“搜索空间的区域。

</details>

#### 检查训练曲线

<details><summary><em>[点击展开]</em></summary>


<br>


***概括：*** *检查训练曲线是识别常见故障模式的一种简单方法，可以帮助我们确定下一步要采取的行动的优先级。*

-   虽然在许多情况下，我们实验的主要目标只需要考虑每次试验的验证错误，但在将每次试验减少到单个数字时我们必须小心，因为它可以隐藏表面下发生的事情的重要细节。
-   对于每项研究，我们总是至少查看少数最佳试验的**训练曲线**（绘制的训练误差和验证误差与训练期间训练步骤的关系图）。
-   即使这不是解决主要实验目标所必需的，检查训练曲线也是识别常见故障模式的一种简单方法，并且可以帮助我们确定下一步要采取的行动的优先级。
-   在检查训练曲线时，我们对以下问题感兴趣。
-   是否有任何试验显示**有问题的过度拟合？**
    -   当验证错误在训练期间的某个时刻开始*增加*时，就会出现有问题的过度拟合。
    -   在我们通过为科学超参数的每个设置选择“最佳”试验来优化讨厌的超参数的实验设置中，我们应该检查*至少*每个与设置相对应的最佳试验中的过度拟合问题我们正在比较的科学超参数。
        -   如果任何最佳试验出现过拟合问题，我们通常希望在比较科学超参数的值之前使用额外的正则化技术重新运行实验和/或更好地调整现有的正则化参数。
            -   如果科学超参数包括正则化参数，这可能不适用，因为如果这些正则化参数的低强度设置导致有问题的过度拟合，也就不足为奇了。
        -   使用常见的正则化技术减少过度拟合通常很简单，这些技术增加了最小的代码复杂性或额外的计算（例如，丢失、标签平滑、权重衰减），因此在下一轮实验中添加一个或多个这些通常没什么大不了的。
        -   例如，如果科学超参数是“隐藏层数”，并且使用最大隐藏层数的最佳试验表现出过拟合问题，那么我们通常更愿意使用额外的正则化再次尝试，而不是立即选择较小数量的隐藏层。
        -   即使“最佳”试验都没有表现出有问题的过度拟合，如果它发生在*任何*试验中，仍然可能存在问题。
            -   选择最佳试验会抑制出现过拟合问题的配置，并偏向那些不会出现过拟合问题的配置。换句话说，它将支持具有更多正则化的配置。
            -   然而，任何让训练变得更糟的事情都可以作为正则化器，即使它不是故意的。例如，选择较小的学习率可以通过阻碍优化过程来规范训练，但我们通常不希望以这种方式选择学习率。
            -   因此，我们必须意识到，科学超参数的每个设置的“最佳”试验可能会以有利于某些科学或令人讨厌的超参数的“坏”值的方式选择。
-   训练后期的训练或验证错误是否存在高步进方差？
    -   如果是这样，这可能会干扰我们比较科学超参数的不同值的能力（因为每个试验随机地以“幸运”或“不幸”步骤结束）以及我们在生产中重现最佳试验结果的能力（因为生产模型可能不会以与研究中相同的“幸运”步骤结束）。
    -   步进方差的最可能原因是批次方差（从每个批次的训练集中随机抽取示例）、小验证集以及在训练后期使用过高的学习率。
    -   可能的补救措施包括增加批量大小、获取更多验证数据、使用学习率衰减或使用 Polyak 平均。
-   训练结束时试验是否仍在改进？
    -   如果是这样，这表明我们在[“计算限制”制度](#determining-the-number-of-steps-for-each-training-run)中，我们可能会受益于[增加训练步数](#Deciding-how-long-to-train-when-training-is-compute-bound)或更改学习率计划。
-   训练集和验证集的性能在最后的训练步骤之前很久就饱和了吗？
    -   如果是这样，这表明我们处于[“不受计算限制”](#determining-the-number-of-steps-for-each-training-run)制度中，我们可能能够[减少训练步数](#deciding-how-long-to-train-when-training-is-not-compute-bound)。
-   虽然我们不能一一列举，但还有许多其他行为可以通过检查训练曲线变得明显（例如，训练期间的训练损失*增加*通常表示训练管道中存在错误）。

</details>

#### 使用隔离图检测更改是否有用

<details><summary><em>[点击展开]</em></summary>


<br>


<p align="center" id="figure-2">
<img src="assets/isolation_plot.png" width="49%" alt="Isolation plot that investigates the best value of weight decay for ResNet-50 trained on ImageNet.">
</p>


<p align="center"><b>图 2：</b>研究在 ImageNet 上训练的 ResNet-50 的最佳权重衰减值的隔离图。</p>

-   通常，一组实验的目标是比较科学超参数的不同值。
    -   例如，我们可能想要确定导致最佳验证错误的权重衰减值。
-   **孤立地块**是基本超参数轴图的特例。隔离图上的每个点都对应于*最好*试验在一些（或所有）讨厌的超参数上的表现。
    -   换句话说，我们绘制了“优化掉”讨厌的超参数后的模型性能。
-   隔离图可以更轻松地在科学超参数的不同值之间进行同类比较。
-   例如，[Figure 2](#figure-2)揭示了权重衰减的值，该值可为在 ImageNet 上训练的 ResNet-50 的特定配置产生最佳验证性能。
    -   如果我们的目标是确定是否完全包括权重衰减，那么我们会将此图中的最佳点与没有权重衰减的基线进行比较。为了公平比较，基线的学习率也应该同样得到很好的调整。
-   当我们有（准）随机搜索生成的数据并考虑隔离图的连续超参数时，我们可以通过对基本超参数轴图的 x 轴值进行分桶并在每个垂直切片中进行最佳试验来近似隔离图由桶定义。

</details>

#### 自动化一般有用的图

<details><summary><em>[点击展开]</em></summary>


<br>

-   生成图的努力越多，我们就越不可能尽可能多地查看它们，因此我们有必要设置我们的基础设施以自动生成尽可能多的图。
-   至少，我们会为我们在实验中变化的所有超参数自动生成基本超参数轴图。
-   此外，我们会自动为所有试验生成训练曲线，并尽可能轻松地找到每项研究中最好的少数试验并检查它们的训练曲线。
-   我们可以添加许多其他有用的潜在图表和可视化。尽管上面描述的是一个很好的起点，但套用杰弗里 · 辛顿 (Geoffrey Hinton) 的话，“每次设计新事物时，您都会学到新事物。”

</details>

### 确定是否采用训练管道更改或超参数配置

***概括：*** *在决定是否更改我们的模型或训练程序或采用新的超参数配置时，我们需要了解结果的不同来源。*

-   当我们尝试改进我们的模型时，我们可能会观察到与现有配置相比，特定的候选更改最初实现了更好的验证错误，但发现在重复实验后并没有一致的优势。非正式地，我们可以将可能导致这种不一致结果的最重要的变异来源分为以下几大类：
    -   **培训程序差异**、**重新训练方差**或**试验方差**：我们在使用相同超参数但使用不同随机种子的训练运行之间看到的变化。
        -   例如，不同的随机初始化、训练数据洗牌、丢弃掩码、数据扩充操作的模式以及并行算术操作的顺序，都是试验方差的潜在来源。
    -   **超参数搜索方差**或**研究方差**：由我们选择超参数的程序引起的结果变化。
        -   例如，我们可能会在特定搜索空间运行相同的实验，但使用两个不同的种子进行准随机搜索，并最终选择不同的超参数值。
    -   **数据收集和抽样方差**：任何类型的随机拆分为训练、验证和测试数据的方差，或更普遍的训练数据生成过程引起的方差。
-   比较使用严格的统计测试在有限验证集上估计的验证错误率是很好的，但通常单独的试验方差可以在使用相同超参数设置的两个不同训练模型之间产生统计显着差异。
-   当我们试图得出超出超参数空间中单个点水平的结论时，我们最关心的是研究方差。
    -   研究方差取决于试验次数和搜索空间，我们已经看到它大于试验方差的情况以及它小得多的情况。
-   因此，在采用候选更改之前，请考虑将最佳试验运行 N 次以表征每次运行的试验方差。
    -   通常，我们可以在对管道进行重大更改后仅重新表征试验方差，但在某些应用程序中，我们可能需要更新的估计值。
    -   在其他应用程序中，表征试验方差的成本太高，不值得。
-   归根结底，虽然我们只想采用能够产生真正改进的更改（包括新的超参数配置），但要求完全确定某些东西有帮助也不是正确的答案。
-   因此，如果一个新的超参数点（或其他变化）得到比基线更好的结果（尽可能考虑新点和基线的再训练方差），那么我们可能应该采用它作为新的基线为以后的比较。
    -   但是，我们应该只采用产生的改进超过它们增加的任何复杂性的更改。

### 探索结束后

***概括：*** *一旦我们完成了对良好搜索空间的探索并决定了应该调整哪些超参数，贝叶斯优化工具就是一个引人注目的选择。*

-   在某个时候，我们的优先事项将从更多地了解调优问题转移到生成单一最佳配置以启动或以其他方式使用。
-   在这一点上，应该有一个精确的搜索空间，可以舒适地包含最佳观察试验周围的局部区域，并且已经过充分采样。
-   我们的探索工作应该已经揭示了最重要的要调整的超参数（以及它们的合理范围），我们可以使用这些超参数来构建搜索空间，以使用尽可能大的调整预算进行最终的自动调整研究。
-   由于我们不再关心最大化我们对调整问题的洞察力，许多[准随机搜索的优点](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)不再适用，应该使用贝叶斯优化工具来自动找到最佳超参数配置。
    -   如果搜索空间包含大量发散点（获得 NaN 训练损失或什至训练损失比均值差很多标准差的点），使用黑盒优化工具来正确处理发散试验很重要（请参阅[具有未知约束的贝叶斯优化](https://arxiv.org/abs/1403.5607)是处理此问题的绝佳方法）。
-   此时，我们还应该考虑检查测试集上的性能。
    -   原则上，我们甚至可以将验证集折叠到训练集中，并重新训练通过贝叶斯优化找到的最佳配置。但是，这仅适用于未来不会针对此特定工作负载发布的情况（例如，一次性 Kaggle 竞赛）。

## 确定每次训练运行的步数

-   有两种类型的工作负载：计算密集型和非计算密集型工作负载。
-   当训练为**受计算限制**时，训练受限于我们愿意等待的时间，而不是受我们拥有多少训练数据或其他因素的限制。
    -   在这种情况下，如果我们能够以某种方式训练更长时间或更有效，我们应该会看到更低的训练损失，并且通过适当的调整，可以改善验证损失。
    -   换句话说，*加速中*训练等同于*改善*训练，“最佳”训练时间始终是“只要我们负担得起”。
    -   也就是说，仅仅因为工作负载受计算限制并不意味着更长/更快的训练是改善结果的唯一途径。
-   当训练为**不受计算限制**时，我们可以负担得起训练的时间，只要我们愿意，并且在某些时候，训练更长的时间并没有多大帮助（甚至会导致有问题的过度拟合）。
    -   在这种情况下，我们应该期望能够训练到非常低的训练损失，训练时间更长可能会略微减少训练损失，但不会显着减少验证损失。
    -   特别是当训练不受计算限制时，更慷慨的训练时间预算可以使调整更容易，特别是在调整学习率衰减计划时，因为它们与训练预算有特别强的相互作用。
        -   换句话说，非常吝啬的训练时间预算可能需要将学习率衰减计划调整到完美，以实现良好的错误率。
-   无论给定的工作负载是否受计算限制，增加梯度方差（跨批次）的方法通常会导致训练进度变慢，因此可能会增加达到特定验证损失所需的训练步骤数。高梯度方差可能由以下原因引起：
    -   使用较小的批量大小
    -   添加数据扩充
    -   添加一些类型的正则化（例如 dropout）

### 当训练受*不是*计算限制时决定训练多长时间

-   我们的主要目标是确保我们训练足够长的时间以使模型达到最佳结果，同时避免在训练步骤的数量上过度浪费。
-   如有疑问，宁可延长训练时间。假设追溯（最佳）检查点选择被正确使用并且检查点足够频繁，那么在训练时间更长时性能永远不会降低。
-   切勿在研究中调整 `max_train_steps` 数字。选择一个值并将其用于所有试验。从这些试验中，绘制回顾性检查点选择找到的训练步骤，以优化 `max_train_steps` 的选择。
    -   例如，如果最佳步骤总是在训练的前 10% 期间，那么最大步骤数就太高了。
    -   或者，如果最佳步骤始终出现在最后 25% 的训练中，我们可能会受益于更长时间的训练和重新调整衰减时间表。
-   当架构或数据发生变化（例如添加数据扩充）时，训练步骤的理想数量可能会发生变化。
-   下面我们将描述如何根据使用恒定学习率“完美拟合”训练集所需的步数，为 `max_train_steps` 选择初始候选值。
    -   请注意，我们并没有以精确或数学上明确定义的方式使用短语“完全适合训练集”。它只是一个非正式的描述符，表示非常低的训练损失。
        -   例如，当使用对数损失进行训练时，缺少正则化项，我们可能会看到训练损失一直在缓慢改善，直到我们达到浮点数限制，因为网络权重无限制地增长并且模型对训练集的预测变得越来越有信心。在这种情况下，我们可以说模型在错误分类误差在训练集上达到零时“完全适合”训练集。
    -   如果训练过程中的梯度噪声量增加，我们发现 `max_train_steps` 的起始值可能需要增加。
        -   例如，如果将数据增强或正则化器（如 dropout）引入模型。
    -   如果训练过程有所改进，则可能会减少 `max_train_steps`。
        -   例如，使用更好调整的优化器或更好调整的学习率计划。

#### 使用学习率扫描为 max_train_steps 选择初始候选者的算法

<details><summary><em>[点击展开]</em></summary>


<br>

-   此过程假设不仅可以“完美”地拟合训练集，而且可以使用恒定的学习率计划来实现。
-   如果可以完美地拟合整个训练集，则必须存在一个完美地拟合训练集的配置（具有 `max_train_steps` 的某个值）；找到任何此类配置并使用其值 `max_train_steps` 作为起点 `N`。
-   在没有数据增强和正则化的情况下运行恒定的学习率扫描（即网格搜索学习率），其中每个试验训练 `N` 步骤。
-   扫描中最快的试验达到完美训练性能所需的步数是我们对 `max_train_steps` 的初步猜测。
-   **笔记：**错误的搜索空间可能会导致自欺欺人。
    -   例如，如果一项研究中的所有学习率都太小，我们可能会错误地得出结论认为需要非常大的 `max_train_steps` 值。
    -   至少，我们应该检查研究中的最佳学习率是否不在搜索空间的边界。

</details>

### 当训练受计算限制时决定训练多长时间

-   在某些情况下，训练损失会无限期地改善，而我们的耐心和计算资源成为限制因素。
-   如果训练损失（或什至验证损失）无限期地改善，我们是否应该在我们负担得起的情况下一直训练？不必要。
    -   我们可以通过运行大量较短的实验并为我们希望推出的模型保留最长的“生产长度”运行来更有效地进行调整。
    -   随着试验的训练时间接近我们的耐心极限，调优实验与我们潜在的发射候选者变得更加相关，但我们可以完成的实验更少。
    -   仅训练约 10% 的生产长度时，我们可能可以回答很多问题，但始终存在这样的风险，即我们在这个时间限制下的结论不适用于生产长度的 20% 的实验，更不用说 100% 了.
-   随着每次试验训练步数限制的增加，在多轮中进行调整是一种明智的方法。
    -   我们可以想做多少轮就做多少轮，但通常 1-3 轮是最实用的。
    -   从本质上讲，尝试使用具有非常快的周转时间的试验来尽可能多地了解问题，权衡调整彻底性与最终最长运行的相关性。
    -   一旦给定的每次试验时间限制产生了有用的见解，我们就可以增加训练时间并继续调整，根据需要从较短的运行中仔细检查我们的结论。
-   作为起点，我们建议进行两轮调整：
    -   第 1 轮：较短的运行时间以找到好的模型和优化器超参数。
    -   第 2 轮：很少在良好的超参数点上长时间运行以获得最终模型。
-   从 `Round i` → `Round i+1` 的最大问题是如何调整学习率衰减时间表。
    -   在轮次之间调整学习率计划时，一个常见的陷阱是使用所有额外的训练步骤而学习率太小。

#### 第 1 轮

<details><summary><em>[点击展开]</em></summary>


<br>

-   不幸的是，当训练长度显着增加时，不能保证在短的、不完整的训练中找到的好的超参数仍然是好的选择。然而，对于某些类型的超参数，它们通常具有足够的相关性，因此第 1 轮非常有用。
-   我们希望将在较短运行中发现的哪些超参数值转移到较长的训练运行中？对于所有这些，我们需要更多的研究。但根据我们目前所知道的，以下是作者的怀疑，按照转移概率的递减顺序：
    -   极有可能转移
        -   早期训练的不稳定性可以在第一轮调整中使用较少的训练步骤来解决。也许这些超参数是我们拥有的最接近确定转移赌注的东西。
            -   热身时长
            -   初始化
    -   可能转移
        -   模型架构——模型架构的戏剧性胜利通常会转移，但可能有很多反例。
    -   可能会转移
        -   优化算法/优化器超参数——我们认为这会“松散地”迁移。绝对比上面的东西要弱。
        -   数据扩充
        -   正则化
            -   如果不可能完美地拟合训练集，则模型可能处于正则化不太可能有太大帮助的状态。
    -   不太可能转移
        -   学习率时间表：不太可能完美迁移。
            -   [This paper](https://arxiv.org/abs/2203.15556)表明即使是延迟调度传输，但我们不认为这在一般情况下是正确的。示例：在小 # 个训练步骤上调整 sqrt 衰减然后扩展到大 # 将导致大部分训练以过小的步骤进行。
                -   在极端培训预算的限制下，大多数计划可能会做得“足够好”，但如果对其进行调整，则可能会看到明显的性能改进。
            -   [了解随机元优化中的短期偏差](https://arxiv.org/abs/1803.02021)描述了尝试短视地选择学习率的危险。

</details>

#### 第二轮

<details><summary><em>[点击展开]</em></summary>


<br>

-   运行第 1 轮的最佳超参数配置。
-   **（推测）**🤖使用额外的步骤来延长高学习率的训练时间。
    -   例如，如果线性计划则保持衰减的长度从第 1 轮开始固定，并在开始时延长恒定 lr 的周期。
    -   对于余弦衰减，只需保留第 1 轮的基础 lr 并像[龙猫纸](https://arxiv.org/abs/2203.15556)一样扩展 `max_train_steps`。
-   对于拥有非常成熟的建模和调整管道以及非常长且昂贵的生产培训运行的团队来说，更多轮次可能是有意义的，但他们往往会矫枉过正。
    -   我们已经描述了如何从步骤 1 →步骤 2 进行转换。如果我们不关心分析时间并且如果高效使用计算是最重要的问题，那么理想的做法是成倍增加训练运行的长度（并且因此完成一项研究的端到端时间）在许多不同轮次的调整中。
        -   在每一轮中，我们都会系统地确保我们的选择继续有效。
        -   新想法通过从步骤 i 到步骤 i+1 的运行时间越来越长的实验逐渐降低风险的管道。

</details>

## 训练流水线的额外指导

### 优化输入管道

***概括：*** *输入绑定管道的原因和干预是高度任务依赖的；使用探查器并寻找常见问题。*

-   使用适当的探查器来诊断输入绑定管道。例如，[Perfetto](https://jax.readthedocs.io/en/latest/profiling.html)用于 JAX 或[TensorFlow 分析器](https://www.tensorflow.org/guide/profiler)用于 TensorFlow。
-   最终，具体原因和干预措施将高度依赖于任务。更广泛的工程考虑（例如，最小化磁盘占用空间）可能会导致更差的输入管道性能。
-   常见原因：
    -   数据未与训练过程共置，导致 I/O 延迟（这可能发生在通过网络读取训练数据时）。
    -   昂贵的在线数据预处理（考虑离线并保存一次）。
    -   干扰数据管道预取的意外同步障碍。例如，在 CommonLoopUtils ([link](https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291)) 中同步设备和主机之间的指标时。
-   常用技巧：
    -   用于预取示例的仪器输入管道（例如[tf.data.Dataset.预取](https://www.tensorflow.org/guide/data_performance#prefetching)）
    -   尽早从管道中删除未使用的功能/元数据。
    -   增加为输入管道生成示例的作业数量的复制。例如，通过使用[tf.data 服务](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service)。

### 评估模型性能

***概括：*** *以比训练更大的批量运行评估。以固定的步长间隔而不是固定的时间间隔运行评估。*

#### 评估设置

<details><summary><em>[点击展开]</em></summary>


<br>

-   我们可以在多种设置中评估模型的性能。
    -   **在线测评** - 当模型在生产环境中提供预测服务时收集指标。
    -   **离线评估** - 当模型在代表生产环境的离线训练/验证/测试集上运行时收集指标。
    -   **定期评估** - 在模型训练期间收集的指标可能是离线评估的代理，和/或离线评估中使用的数据子集。
-   在线评估是黄金标准，但在模型开发阶段通常不切实际。
-   根据问题的不同，离线评估可能相当复杂且计算量大。
-   定期评估是最实用、最经济的选择，但可能无法完全代表生产环境。
    -   我们在定期评估期间的目标是使用离线评估的权宜之计，而不牺牲我们在训练期间获得的信号的可靠性。

</details>

#### 设置定期评估

<details><summary><em>[点击展开]</em></summary>


<br>

-   我们在训练期间运行定期评估以实时监控其进度，[促进回顾性模型检查点选择](#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint)，这样我们就可以[检查训练结束时的训练曲线](#examining-the-training-curves)。
-   最简单的配置是在同一个计算实例中同时执行训练和定期评估，定期在训练和评估之间交替。
    -   在这种情况下，用于执行评估的批量大小应*至少*与用于训练的批量大小一样大，因为在评估期间不需要维护模型激活，从而降低了每个示例的计算要求.
-   定期评估应该以固定的步长间隔而不是时间间隔进行。
    -   基于时间间隔进行评估会使解释训练曲线变得更加困难，尤其是当训练可能会受到训练作业抢占、网络延迟问题等的影响时。
-   有效/测试指标的周期性（当使用混洗训练/验证/测试拆分时）可以指示实现错误，例如测试数据与训练数据重叠，或训练数据未正确混洗。以固定的步骤间隔进行评估可以使这些问题更容易发现。
-   当评估集不能被批次大小整除时，可能会出现部分批次。确保正确权衡填充的示例，以防止损失函数受到它们的影响。通常，这些填充示例的权重可以为零。
-   每次评估保存足够的信息以支持离线分析。理想情况下，我们会保存对选定的单个示例的预测，因为它们对于调试来说非常宝贵。
    -   生成像[保存模型](https://www.tensorflow.org/guide/saved_model)这样的工件可以在评估工作完成后轻松进行临时模型检查。

</details>

#### 选择样本进行定期评估

<details><summary><em>[点击展开]</em></summary>


<br>

-   定期评估作业的运行速度可能不够快，无法在合理的时间内计算完整离线评估集的指标。这通常需要抽样数据进行定期评估。
-   在构建样本数据集时，我们考虑以下因素：
    -   <ins>样本量</ins>
        -   检查在周期性作业使用的采样数据集上计算的性能是否与整个离线评估集的性能相匹配，即采样集和完整数据集之间没有偏差。
        -   用于定期评估的数据集应该足够小，以便可以轻松生成整体模型预测，但也应该足够大，以便可以准确测量模型的改进（即不会被标签噪声淹没）。
        -   它应该足够大，可以按顺序在试验中容纳多个此类评估，并且仍然可以产生准确的估计。也就是说，为了避免随着时间的推移自适应地“适应”验证集，这种方式不会推广到保留的测试集。然而，这种考虑很少是一个实际问题。
    -   <ins>不平衡的数据集</ins>
        -   对于不平衡的数据集，稀有类别示例的性能通常会很嘈杂。
        -   对于类标签中包含少量示例的数据集，记录正确预测的示例数量，以更深入地了解准确性改进（.05 灵敏度改进听起来令人兴奋，但它只是一个正确的示例吗？）。

</details>

### 保存检查点并回溯选择最佳检查点

***概括：*** *运行固定步数的训练，并回顾性地从运行中选择最佳检查点。*

-   大多数深度学习框架都支持[模型检查点](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html)。也就是说，模型的当前状态会定期保存在磁盘上。这允许训练作业对计算实例中断具有弹性。
-   最佳检查点通常不是最后一个检查点，尤其是当验证集性能不会随时间持续增加而是围绕特定值波动时。
-   设置管道以跟踪到目前为止在训练期间看到的 N 个最佳检查点。在训练结束时，模型选择就是选择训练期间看到的最佳检查点。我们称之为**回顾性最佳检查点选择**。
-   支持预期提前停止通常是不必要的，因为我们预先指定了试验预算并保留了迄今为止看到的 N 个最佳检查点。

### 设置实验跟踪

***概括：*** *在跟踪不同的实验时，请务必注意一些要点，例如研究中检查点的最佳性能以及研究的简短描述。*

-   我们发现，在电子表格中跟踪实验结果有助于我们解决各种建模问题。它通常有以下列：
    -   研究名称
    -   指向研究配置存储位置的链接。
    -   研究的注释或简短描述。
    -   试运行次数
    -   研究中最佳检查点在验证集上的表现。
    -   关于启动培训所必需的未提交更改的特定复制命令或注释。
-   找到一个跟踪系统，该系统至少可以捕获上面列出的信息，并且对执行此操作的人来说很方便。未跟踪的实验也可能不存在。

### 批量归一化实现细节

***概括：*** *现在 batch norm 通常可以用 LayerNorm 代替，但在不能代替的情况下，更改批处理大小或主机数量时会出现一些棘手的细节。*

-   Batch norm 使用当前批次的均值和方差对激活进行归一化，但在多设备设置中，除非明确同步，否则这些统计数据在每个设备上都是不同的。
-   轶事报告（主要在 ImageNet 上）说仅使用约 64 个示例计算这些规范化统计数据实际上在实践中效果更好（请参阅[this paper](https://arxiv.org/abs/1705.08741)的 Ghost Batch Norm）。
-   将总批大小与用于计算批归一化统计的示例数量分离对于批大小比较特别有用。
-   Ghost batch norm 实现并不总能正确处理每台设备的批处理大小 > 虚拟批处理大小的情况。在这种情况下，我们实际上需要在每个设备上对批次进行二次抽样，以获得适当数量的批次规范统计示例。
-   测试模式 batch norm 中使用的指数移动平均只是训练统计数据的线性组合，因此这些 EMA 只需要在将它们保存在检查点之前进行同步。但是，批归一化的一些常见实现不同步这些 EMA，只保存来自第一个设备的 EMA。

### 多主机管道的注意事项

***概括：*** *对于日志记录、评估、RNG、检查点和数据分片，多主机训练可以很容易地引入错误！*

-   确保管道仅在一台主机上记录和检查点。
-   确保在运行评估或检查点之前，批处理规范统计信息在主机之间同步。
-   具有跨主机相同的 RNG 种子（用于模型初始化）和跨主机不同的种子（用于数据改组/预处理）至关重要，因此请确保适当地标记它们。
-   通常建议跨主机分片数据文件以提高性能。

## 常见问题

### 最好的学习率衰减时间表系列是什么？

<details><summary><em>[点击展开]</em></summary>


<br>

-   这是一个悬而未决的问题。目前尚不清楚如何构建一组严格的实验来自信地回答“最佳”LR 衰减时间表是什么。
-   虽然我们不知道最好的时间表系列，但我们相信拥有一些（非恒定的）时间表很重要并且调整它很重要。
-   在优化过程中，不同的学习率在不同的时间效果最好。有某种时间表可以使模型更有可能达到良好的学习率。

</details>

### 我应该使用哪种学习率衰减作为默认值？

<details><summary><em>[点击展开]</em></summary>
<br>


-   我们的偏好是线性衰减或余弦衰减，其他一些时间表系列可能也不错。

</details>

### 为什么有些论文有复杂的学习率表？

<details><summary><em>[点击展开]</em></summary>
<br>


-   具有复杂的分段学习率 (LR) 衰减时间表的论文并不少见。
-   读者常常想知道作者是如何得出如此复杂的研究结果的。
-   许多复杂的 LR 衰减时间表是根据验证集性能以临时方式调整时间表的结果：
    1.  使用一些简单的 LR 衰减（或恒定学习率）开始单次训练运行。
    2.  继续训练，直到表现似乎停滞不前。如果发生这种情况，请暂停训练。从这一点开始，使用可能更陡峭的 LR 衰减时间表（或更小的恒定学习率）恢复它。重复此过程，直到会议/发布截止日期。
-   愉快地复制生成的*日程*通常不是一个好主意，因为最佳的特定时间表将对许多其他超参数选择敏感。
    -   最好复制生成时间表的*算法*，尽管在​​人为判断生成时间表时这几乎不可能。
-   如果这种类型的验证错误敏感计划可以完全自动化，则可以很好地使用，但作为验证错误函数的人在循环计划是脆弱的并且不容易重现，因此我们建议避免使用它们。
    -   在发布使用此类时间表的结果之前，请尝试使其完全可重现。

</details>

### Adam 的超参数应该如何调整？

<details><summary><em>[点击展开]</em></summary>
<br>


-   如上所述，对搜索空间以及应该从搜索空间中采样多少点做出一般性陈述是非常困难的。请注意，并非 Adam 中的所有超参数都同等重要。以下经验法则对应于研究中试验次数的不同“预算”。
    -   如果一项研究中有 < 10 次试验，则只调整（基础）学习率。
    -   如果 10-25 次试验，调整学习率和$\beta_1$。
    -   如果超过 25 次试验，调整学习率，$\beta_1$和$\epsilon$。
    -   如果可以运行超过 25 次试验，则另外调整$\beta_2$。

</details>

### 为什么在优化的探索阶段使用准随机搜索而不是更复杂的黑盒优化算法？

<details><summary><em>[点击展开]</em></summary>


-   准随机搜索（基于[低差异序列](https://en.wikipedia.org/wiki/Low-discrepancy_sequence)）是我们在用作迭代调优过程的一部分时优于更高级的黑盒优化工具，旨在最大限度地洞察调优问题（我们称之为“探索阶段”）。贝叶斯优化和类似工具更适合开发阶段。
-   基于随机移动的低差异序列的准随机搜索可以被认为是“抖动的、打乱的网格搜索”，因为它统一但随机地探索给定的搜索空间，并比随机搜索更分散搜索点。
-   与更复杂的黑盒优化工具（例如贝叶斯优化、进化算法）相比，准随机搜索的优势包括：
    1.  非自适应地对搜索空间进行采样可以在不重新运行实验的情况下更改事后分析中的调整目标。
        -   例如，我们通常希望根据训练中任何一点的验证误差找到最佳试验。但准随机搜索的非自适应特性使得无需重新运行任何实验即可根据最终验证误差、训练误差或某些替代评估指标找到最佳试验成为可能。
    2.  准随机搜索以一致且统计上可重现的方式运行。
        -   即使搜索算法的实施发生变化，也应该可以重现六个月前的研究，只要它保持相同的均匀性。如果使用复杂的贝叶斯优化软件，实现可能会在版本之间发生重大变化，从而使旧搜索更难重现。回滚到旧的实现并不总是可能的（例如，如果优化工具作为服务运行）。
    3.  它对搜索空间的统一探索使得对结果以及它们可能对搜索空间提出的建议的推理变得更容易。
        -   例如，如果准随机搜索遍历中的最佳点位于搜索空间的边界，这是一个很好（但不是万无一失）的信号，表明应该更改搜索空间边界。[这个部分](#identifying-bad-search-space-boundaries)更深入。然而，自适应黑盒优化算法可能会因为一些不幸的早期试验而忽略了搜索空间的中间部分，即使它恰好包含同样好的点，因为正是这种不均匀性正是一个好的优化算法所需要的雇用加快搜索。
    4.  与自适应算法不同，在使用准随机搜索（或其他非自适应搜索算法）时，并行运行与顺序运行不同数量的试验不会产生统计上不同的结果。
    5.  更复杂的搜索算法可能并不总能正确处理不可行的点，特别是如果它们在设计时未考虑神经网络超参数调整。
    6.  准随机搜索很简单，并且在许多调整试验将并行运行时特别有效。
        -   有趣的是 [^3]，自适应算法很难击败预算是其两倍的准随机搜索，尤其是当许多试验需要并行运行时（因此很少有机会利用先前的启动新试验时的试验结果）。
        -   如果没有贝叶斯优化和其他高级黑盒优化方法方面的专业知识，我们可能无法获得它们原则上能够提供的好处。在现实的深度学习调整条件下，很难对高级黑盒优化算法进行基准测试。它们是当前研究中非常活跃的领域，对于没有经验的用户来说，更复杂的算法也有其自身的缺陷。这些方法的专家能够获得良好的结果，但在高并行条件下，搜索空间和预算往往更为重要。
-   也就是说，如果我们的计算资源只允许少量试验并行运行，而我们有能力按顺序运行许多试验，那么贝叶斯优化就会变得更有吸引力，尽管这会使我们的调整结果更难解释。

[^3]：Ben Recht 和 Kevin Jamieson

</details>

### 在哪里可以找到准随机搜索的实现？

<details><summary><em>[点击展开]</em></summary>
<br>


-   我们使用[这个实现](https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py)为给定的搜索空间生成一个 Halton 序列（旨在实现https://arxiv.org/abs/1706.03200).中推荐的移位、加扰的 Halton 序列
-   如果基于低差异序列的准随机搜索算法不可用，则可以替代伪随机均匀搜索，尽管这可能效率稍低。
    -   在 1-2 维中，网格搜索也是可以接受的，尽管在更高的维度中不行（参见[伯格斯特拉和本吉奥，2012](https://www.jmlr.org/papers/v13/bergstra12a.html)）。

</details>

### 需要多少次试验才能通过准随机搜索获得好的结果？

<details><summary><em>[点击展开]</em></summary>
<br>


<p align="center">
<img src="assets/have_we_sampled_enough.png" width="49%" alt="A box plot showing the importance of sampling enough">
</p>


<p align="center"><b>图 3：</b> ResNet-50 在 ImageNet 上进行了 100 次试验调整。通过自举，模拟了不同数量的调整预算。上面绘制了每个试验预算的最佳性能的箱线图。


-   这个问题没有办法笼统地回答，但是我们可以看具体的例子。
-   如图 3 所示，研究中的试验次数会对结果产生重大影响。
    -   请注意，当对 6 个试验进行抽样时，与对 20 个试验进行抽样时的四分位间距有多大。
    -   即使进行了 20 次试验，特别幸运和不幸的研究之间的差异也可能大于使用固定超参数在不同随机种子上重新训练该模型之间的典型差异，对于此工作负载可能约为 +/- 0.1% 的验证错误率为 \~23%。

</details>

### 如何调试和缓解优化失败？

<details><summary><em>[点击展开]</em></summary>
<br>



***概括：*** *如果模型遇到优化困难，那么在尝试其他事情之前解决这些问题很重要。诊断和纠正培训失败是一个活跃的研究领域。*

<p align="center">
<img src="assets/stride_instability.png" width="80%" alt="Changing the strides in a single residual block in a WideResnet results in training instability.">
</p>



<p align="center"><b>图 4：</b>在 WideResnet 中更改单个残差块 (2x2 -> 1x1) 中的步幅会导致训练不稳定。这不会降低低学习率下的性能，但由于不稳定，高学习率不再能很好地训练。应用 1000 步的学习率预热解决了这种特殊的不稳定情况，允许以 0.1 的最大学习率进行稳定训练。</p>

#### 识别不稳定的工作负载

-   如果学习率太大，任何工作负载都会变得不稳定。只有当它迫使我们使用太小的学习率时，不稳定才是一个问题。
-   至少有两种类型的训练不稳定性值得区分：
    1.  初始化/训练早期不稳定。
    2.  训练中途突然不稳定。
-   我们可以采用系统的方法来识别工作负载中的稳定性问题。
    1.  做一个学习率扫描，找到最好的学习率 lr*。
    2.  为刚好高于 lr* 的学习率绘制训练损失曲线。
    3.  如果学习率 > lr* 显示损失不稳定性（损失在训练期间上升而不下降），那么修复不稳定性可能会导致更好的训练。
-   在训练过程中记录全损失梯度的 L2 范数，异常值会导致训练过程中的虚假不稳定性。这可以告知如何选择渐变/更新剪辑。

**笔记：**一些模型在早期表现出不稳定，随后出现恢复，导致缓慢但稳定的训练。**常见的评估计划可能会因为评估不够频繁而错过这些问题！**

为了检查这一点，我们可以使用 `lr = 2 * current best` 训练仅 \~500 步的简短运行，但评估每一步。

<p align="center">
<img src="assets/more_frequent_evals.png" width="80%" alt="Illustration of the value of more frequent evaluations at the start of training.">
</p>


<p align="center"><b>图 5：</b>训练开始时更频繁评估的价值说明。如果怀疑模型受到早期训练不稳定的影响，则很有用。</p>

#### 常见不稳定模式的潜在修复

-   应用学习率预热
    -   最适合早期训练不稳定。
-   应用渐变剪裁
    -   对早期和中期训练不稳定都有好处，可能会修复一些热身无法解决的问题。
-   尝试新的优化器
    -   有时 Adam 可以处理 Momentum 无法处理的不稳定性。这是一个活跃的研究领域。
-   我们可以确保我们正在为我们的模型架构（下面的示例）使用最佳实践/初始化。
    -   如果模型尚未包含剩余连接和规范化，则添加它。
-   归一化应该是残差之前的最后一个操作。例如 x + Norm(f(x))。
-   Norm(x + f(x)) 已知会导致问题。
-   尝试将剩余分支初始化为 0（例如[重新调零初始化](https://arxiv.org/abs/2003.04887)）。
-   降低学习率
    -   这是最后的手段。

#### 学习率预热

<p align="center">
<img src="assets/instability_during_warmup.png" width="80%" alt="An example of instability during a warmup period (note the horizontal axis log scale).">
</p>


<p align="center"><b>图 6：</b>预热期间不稳定的示例（注意水平轴对数刻度）。在这种情况下，成功训练需要 40k 步的热身。</p>

##### 何时应用学习率预热

<p align="center">
<img src="assets/axis_model_with_instability.png" width="49%" alt="Axis plot for model with instability">
</p>


<p align="center"><b>图 7a：</b>展示训练不稳定性的模型的超参数轴图示例。最佳学习率处于可行的边缘。“不可行”试验被定义为产生 NaN 或异常高的损失值的试验。</p>

<p align="center">
<img src="assets/loss_model_with_instability.png" width="49%" alt="Loss curve for model with instability">
</p>


<p align="center"><b>图 7b：</b>以我们看到的不稳定学习率训练的模型的训练损失。</p>

-   图 7a 显示了一个超参数轴图，该图表明模型正在经历优化不稳定性，因为最佳学习率恰好位于不稳定的边缘。
-   图 7b 显示了如何通过检查以比该峰值大 5 倍或 10 倍的学习率训练的模型的训练损失来双重检查。如果该图显示损失在稳步下降后突然上升（例如上图中的步长 ~10k），则该模型可能存在优化不稳定性。

##### 如何应用学习率预热

<p align="center">
<img src="assets/beneficial_effect_warmup.png" width="80%" alt="Beneficial effect of warmup on training instabilities">
</p>


<p align="center"><b>图 8：</b>学习率预热对解决训练不稳定性的有益影响。</p>

-   使用上面的部分，我们假设从业者已经确定了模型变得不稳定的学习率。这是 `unstable_base_learning_rate`。
-   热身涉及预先安排一个学习率计划，将学习率从 0 提高到某个稳定的 `base_learning_rate`，这至少比 `unstable_base_learning_rate` 大一个数量级。默认设置是尝试 `base_learning_rate`，即 10x `unstable_base_learning_rate`。尽管请注意，对于类似 100x `unstable_base_learning_rate` 的情况，可以再次运行整个过程。具体日程为：
    -   从 0 上升到 `base_learning_rate` 超过<g r="324">。
    -   以恒定速率训练 `post_warmup_steps`。
-   我们的目标是找到最短的 `warmup_steps` 数量，使我们能够获得比 `unstable_base_learning_rate` 高得多的峰值学习率。
-   因此，对于每个 `base_learning_rate`，我们需要调整 `warmup_steps` 和 `post_warmup_steps`。通常将 `post_warmup_steps` 设置为 `2*warmup_steps` 就可以了。
-   预热可以独立于现有的衰减时间表进行调整。 `warmup_steps` 应该以几个不同的数量级扫描。例如，一个示例研究可以尝试 [10, 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>]。最大可行点不应超过<g r="334">的 10%。
-   一旦建立了不会破坏 `base_learning_rate` 训练的 `warmup_steps`，就应该将其应用于基线模型。本质上，我们将这个时间表添加到现有时间表上，并使用上面讨论的最佳检查点选择来将这个实验与基线进行比较。例如，如果我们最初有 10,000 个 `max_train_steps` 并且执行了 `warmup_steps` 1000 步，那么新的训练程序应该总共运行 11,000 步。
-   如果稳定训练需要长 `warmup_steps`（ `max_train_steps` 的 5%），则可能需要增加 `max_train_steps` 来解决这个问题.
-   在整个工作负载范围内并没有真正的“典型”值。有些模型只需要 100 步，而其他模型（尤其是变压器）可能需要 40k+。

#### 渐变剪裁

<p align="center">
<img src="assets/gradient_clipping.png" width="80%" alt="Gradient clipping on early training instabilities">
</p>


<p align="center"><b>图 9：</b>梯度裁剪纠正早期训练不稳定性的图示。</p>

-   当出现大的或离群的梯度问题时，梯度裁剪最有用。
-   裁剪可以修复早期训练的不稳定性（早期的大梯度范数），或中期训练的不稳定性（训练中期突然的梯度尖峰）。
-   有时，较长的预热期可以纠正削波不能纠正的不稳定性：请参阅[上面这一段](#How-to-apply-learning-rate-warmup)。
    -   🤖在预热期间剪裁怎么办？
-   理想的剪辑阈值刚好高于“典型”梯度范数。
-   下面是如何进行渐变裁剪的示例：
    -   如果梯度范数$\left | g \right |$大于梯度裁剪阈值$\lambda$，则执行${g}'= \lambda \times \frac{g}{\left | g \right |}$ where ${g}'$是新的渐变。
-   在训练期间记录未剪切的梯度范数。默认生成：
    -   梯度范数与步长的关系图
    -   在所有步骤上聚合的梯度范数的直方图
-   根据梯度范数的第 90 个百分位数选择梯度裁剪阈值。
    -   阈值将取决于工作负载，但 90% 是一个很好的起点。如果它不起作用，则可以调整此阈值。
    -   🤖某种适应性策略怎么样？
-   如果我们尝试梯度裁剪并且不稳定问题仍然存在，我们可以更努力地尝试（即使阈值更小）。
-   极端激进的梯度裁剪本质上是一种降低学习率的奇怪方式。如果我们发现自己使用了非常激进的裁剪，我们可能应该只降低学习率。
-   我们通常会认为以某种方式将超过 50% 的更新剪裁为“极其激进”。
-   如果我们需要进行极其激进的梯度裁剪来处理我们的不稳定问题，那么我们不妨降低学习率。

</details>

### 为什么将学习率和其他优化参数称为超参数？它们不是任何先验分布的参数。

<details><summary><em>[点击展开]</em></summary>
<br>


-   的确，术语“超参数”在贝叶斯机器学习中有一个精确的[meaning](https://en.wikipedia.org/wiki/Hyperparameter)并且将学习率和我们在深度学习中调整的大多数其他参数称为“超参数”是对术语。
-   我们更愿意使用术语“元参数”来表示学习率、架构参数以及我们在深度学习中调整的所有其他内容，因为它避免了因滥用“超参数”一词而引起的潜在混淆（混淆尤其可能在讨论贝叶斯优化时，概率响应曲面模型有自己的真实超参数）。
-   不幸的是，尽管可能会造成混淆，但超参数这个术语在深度学习社区中已经变得极为普遍。
-   因此，对于像本文档这样的面向广泛受众的文档，其中包括许多不太可能意识到这一技术细节的人，我们选择为该领域的一个混淆来源做出贡献，以期避免另一个混淆来源。
-   也就是说，我们在发表研究论文时可能会做出不同的选择，我们会鼓励其他人在大多数情况下改用“元参数”。

</details>

### 为什么不应该调整批量大小来直接提高验证集性能？

<details><summary><em>[点击展开]</em></summary>
<br>


-   更改批量大小*不改变训练管道的任何其他细节*通常会影响验证集性能。
-   但是，如果针对每个批量大小独立优化训练管道，则两个批量大小之间的验证集性能差异通常会消失。
-   与批量大小交互最强烈的超参数，因此对于每个批量大小单独调整最重要的是优化器超参数（例如学习率、动量）和正则化超参数。
    - 由于样本方差，较小的批次大小会在训练算法中引入更多噪声，并且这种噪声可能具有正则化效果。因此，较大的批量可能更容易过度拟合，并且可能需要更强的正则化和/或额外的正则化技术。
-   此外，[训练步数可能需要调整](#choosing-the-batch-size-to-minimize-training-time)更改批量大小时。
-   一旦考虑了所有这些影响，目前还没有令人信服的证据表明批量大小会影响最大可实现的验证性能（参见[沙鲁等人。2018](https://arxiv.org/abs/1811.03600)）。

</details>

### 所有流行的优化算法的更新规则是什么？

<details><summary><em>[点击展开]</em></summary>


<br>

#### 随机梯度下降 (SGD)

$$\theta_{t+1} = \theta_{t} - \eta_t \nabla \mathcal{l}(\theta_t)$$

#### Momentum

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t v_{t+1}$$

#### 内斯特罗夫

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t( \gamma v_{t+1} + \nabla \mathcal{l}(\theta_{t})$$

#### RMSProp

$$v_0 = 1 \ 文本 {,} m_0 = 0$$

$$v_{t+1} = \rho v_{t} + (1 - \rho) \nabla \mathcal{l}(\theta_t)^2$$

$$m_{t+1} = \gamma m_{t} + \frac{\eta_t}{\sqrt{v_{t+1} + \epsilon}}\nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - m_{t+1}$$

#### ADAM

$$m_0 = 0 \ 文本 {,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l}(\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

#### 那达慕

$$m_0 = 0 \text{,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l} (\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{\beta_1 m_{t+1} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

</details>

## 致谢

-   我们要感谢 Max Bileschi、R ​​ oy Frostig、Zelda Mariet、Stan Bileschi、Mohammad Norouzi、Chris DuBois 和 Charles Sutton 阅读手稿并提供宝贵的反馈。
-   我们重复使用了最初由 Naman Agarwal 为其他联合研究制作的几个地块的一些实验数据。
-   我们要感谢 Will Chen 对文件的介绍提出的宝贵建议。
-   我们还要感谢 Rohan Anil 进行了有益的讨论。

## 引用


```
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}
```

## 贡献

-   这不是官方支持的 Google 产品。

-   我们很想听听您的反馈！

    -   如果您喜欢该剧本，请[留下一颗星星](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository)！或发送电子邮件至 deep-learning-tuning-playbook \[at\] googlegroups.com。推荐书帮助我们证明创建更多这样的资源是合理的。
    -   如果有任何不正确的地方，请提出问题以开始讨论。对于不适合问题的问题或其他消息，请在 GitHub 上打开一个新的讨论主题。

-   正如序言中所讨论的，这是一份动态文件。我们预计会定期进行大大小小的改进。如果您想收到通知，请查看我们的存储库（请参阅[指示](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository)）。

-   请不要在未通过问题跟踪系统与作者协调的情况下提交拉取请求。

### 贡献者许可协议

对该项目的贡献必须附有贡献者许可协议 (CLA)。您（或您的雇主）保留您贡献的版权；这只是允许我们使用和重新分配您的贡献作为项目的一部分。前往<https://cla.developers.google.com/>查看您当前存档的协议或签署新协议。

您通常只需要提交一次 CLA，因此如果您已经提交过一次（即使是针对不同的项目），您可能不需要再次提交。

### 代码审查

所有提交的内容，包括项目成员提交的内容，都需要审查。为此，我们使用 GitHub 拉取请求。有关使用拉取请求的更多信息，请参阅[GitHub 帮助](https://help.github.com/articles/about-pull-requests/)。

### 社区准则

此项目遵循[Google 的开源社区准则](https://opensource.google/conduct/)。
